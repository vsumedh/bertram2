# vLLM Server Configuration
# Copy this file to .env and adjust as needed

# vLLM server endpoint (OpenAI-compatible API)
VLLM_BASE_URL=http://localhost:8000/v1

# Model to use (must match what's loaded in vLLM server)
VLLM_MODEL=Qwen/Qwen2.5-7B-Instruct

# API key (vLLM doesn't require authentication by default)
VLLM_API_KEY=EMPTY

# Request timeout in seconds
VLLM_TIMEOUT=120.0

# White agent settings (optional overrides)
# WHITE_MODEL=Qwen/Qwen2.5-7B-Instruct
# WHITE_TEMPERATURE=0.0
# WHITE_PROMPT_PROFILE=standard
