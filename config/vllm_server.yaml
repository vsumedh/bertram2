# vLLM Server Configuration for TextWorld Agentify
# Model: Qwen2.5-7B-Instruct (~14GB VRAM)

model: Qwen/Qwen2.5-7B-Instruct
host: 0.0.0.0
port: 8000
max_model_len: 8192
gpu_memory_utilization: 0.90
dtype: float16

# Optional settings (uncomment as needed):
# tensor_parallel_size: 1  # For multi-GPU setups
# quantization: null       # "awq", "gptq", "squeezellm" for quantized models
# trust_remote_code: false
# disable_log_requests: true



